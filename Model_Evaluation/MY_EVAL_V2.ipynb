{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from my_evaluation import my_evaluation\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "from copy import deepcopy\n",
    "from pdb import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_evaluation:\n",
    "    # Binary class or multi-class classification evaluation\n",
    "    # Each data point can only belong to one class\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self, predictions, actuals, pred_proba=None):\n",
    "        # inputs:\n",
    "        # predictions: list of predicted classes\n",
    "        # actuals: list of ground truth\n",
    "        # pred_proba: pd.DataFrame of prediction probability of belonging to each class\n",
    "        self.predictions = np.array(predictions)\n",
    "        self.actuals = np.array(actuals)\n",
    "        self.pred_proba = pred_proba\n",
    "        if type(self.pred_proba)!=type(None):\n",
    "            self.classes_ = list(self.pred_proba.keys())\n",
    "        else:\n",
    "            self.classes_ = list(set(list(self.predictions)+list(self.actuals)))\n",
    "        self.confusion_matrix = None\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def confusion(self):\n",
    "        # compute confusion matrix for each class in self.classes_\n",
    "        # self.confusion_matrix = {self.classes_[i]: {\"TP\":tp, \"TN\": tn, \"FP\": fp, \"FN\": fn}}\n",
    "        # no return variables\n",
    "        # write your own code below\n",
    "\n",
    "        correct = self.predictions == self.actuals\n",
    "        wrong = self.predictions != self.actuals\n",
    "        self.acc = float(Counter(correct)[True])/len(correct)\n",
    "        self.confusion_matrix = {}\n",
    "        \n",
    "        for label in self.classes_:\n",
    "            tp = 0\n",
    "            fp = 0\n",
    "            fn = 0\n",
    "            tn = 0\n",
    "            for i in range(len(self.actuals)):\n",
    "                # what is tp ? when actual is setosa and predicted is setosa\n",
    "                if self.predictions[i] == label and self.actuals[i] == label:\n",
    "                    tp = tp + 1\n",
    "                \n",
    "                # what is fp ? when actual is not setosa but predicted is setosa\n",
    "                if self.predictions[i] == label and self.actuals[i] != label:\n",
    "                    fp = fp + 1\n",
    "                \n",
    "                # what is fn ? when actual is setosa but predicted is not setosa\n",
    "                if self.predictions[i] != label and self.actuals[i] == label:\n",
    "                    fn = fn + 1\n",
    "                    \n",
    "                # what is tn ? when actual is not setosa predicted is also not setosa\n",
    "                if self.predictions[i] != label and self.actuals[i] != label:\n",
    "                    tn = tn + 1\n",
    "                \n",
    "                # filling up confusion matrix for each label\n",
    "                self.confusion_matrix[label] = {\"TP\":tp, \"TN\": tn, \"FP\": fp, \"FN\": fn}\n",
    "                \n",
    "                \n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(self.confusion_matrix)\n",
    "        print()\n",
    "        return\n",
    "        \n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    def accuracy(self):\n",
    "        if self.confusion_matrix==None:\n",
    "            self.confusion()\n",
    "        return self.acc\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def precision(self, target=None, average = \"macro\"):\n",
    "        # compute precision\n",
    "        # target: target class (str). If not None, then return precision of target class\n",
    "        # average: {\"macro\", \"micro\", \"weighted\"}. If target==None, return average precision\n",
    "        # output: prec = float\n",
    "        # note: be careful for divided by 0\n",
    "\n",
    "        if self.confusion_matrix==None:\n",
    "            self.confusion()\n",
    "            \n",
    "        if target in self.classes_:\n",
    "            tp = self.confusion_matrix[target][\"TP\"]\n",
    "            fp = self.confusion_matrix[target][\"FP\"]\n",
    "            if tp+fp == 0:\n",
    "                prec = 0\n",
    "            else:\n",
    "                prec = float(tp) / (tp + fp)\n",
    "        else:\n",
    "            if average == \"micro\":\n",
    "                prec = self.accuracy()\n",
    "            else:\n",
    "                prec = 0\n",
    "                n = len(self.actuals)\n",
    "                for label in self.classes_:\n",
    "                    tp = self.confusion_matrix[label][\"TP\"]\n",
    "                    fp = self.confusion_matrix[label][\"FP\"]\n",
    "                    if tp + fp == 0:\n",
    "                        prec_label = 0\n",
    "                    else:\n",
    "                        prec_label = float(tp) / (tp + fp)\n",
    "                    if average == \"macro\":\n",
    "                        ratio = 1 / len(self.classes_)\n",
    "                    elif average == \"weighted\":\n",
    "                        ratio = Counter(self.actuals)[label] / float(n)\n",
    "                    else:\n",
    "                        raise Exception(\"Unknown type of average.\")\n",
    "                    prec += prec_label * ratio\n",
    "        return prec\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def recall(self, target=None, average = \"macro\"):\n",
    "        # compute recall\n",
    "        # target: target class (str). If not None, then return recall of target class\n",
    "        # average: {\"macro\", \"micro\", \"weighted\"}. If target==None, return average recall\n",
    "        # output: recall = float\n",
    "        # note: be careful for divided by 0\n",
    "        \n",
    "        if self.confusion_matrix==None:\n",
    "            self.confusion()\n",
    "\n",
    "        if target in self.classes_:\n",
    "            tp = self.confusion_matrix[target][\"TP\"]\n",
    "            fn = self.confusion_matrix[target][\"FN\"]\n",
    "            if tp + fn == 0:\n",
    "                rec = 0\n",
    "            else:\n",
    "                rec = float(tp) / ( tp + fn )\n",
    "        else:\n",
    "            if average == \"micro\":\n",
    "                rec = self.accuracy()\n",
    "            else:\n",
    "                rec = 0\n",
    "                n = len(self.classes_)\n",
    "                for label in self.classes_:\n",
    "                    tp = self.confusion_matrix[label][\"TP\"]\n",
    "                    fn = self.confusion_matrix[label][\"FN\"]\n",
    "                    if tp + fn == 0:\n",
    "                        rec_label = 0\n",
    "                    else:\n",
    "                        rec_label = float(tp) / (tp + fn )\n",
    "                    if average == \"macro\":\n",
    "                        ratio = 1 / len(self.classes_)\n",
    "                    elif average == \"weighted\":\n",
    "                        ratio = Counter(self.actuals)[label] / float(n)\n",
    "                    else:\n",
    "                        raise Exception(\"Unknown type of average.\")\n",
    "                    rec += rec_label * ratio\n",
    "        return rec\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    def f1(self, target=None, average = \"macro\"):\n",
    "        # compute f1\n",
    "        # target: target class (str). If not None, then return f1 of target class\n",
    "        # average: {\"macro\", \"micro\", \"weighted\"}. If target==None, return average f1\n",
    "        # output: f1 = float\n",
    "        \n",
    "        if self.confusion_matrix == None:\n",
    "            self.confusion()\n",
    "        \n",
    "        if target in self.classes_:\n",
    "            rec = self.recall(target, average)\n",
    "            prec = self.precision(target, average)\n",
    "            \n",
    "            if rec + prec == 0:\n",
    "                f1_score = 0\n",
    "            else:\n",
    "                f1_score = 2 * ((prec * rec) / (prec + rec))\n",
    "        else:\n",
    "            if average == \"micro\":\n",
    "                f1_score = self.accuracy()\n",
    "            else:\n",
    "                f1_score = 0\n",
    "                n = len(self.actuals)\n",
    "                \n",
    "                for label in self.classes_:\n",
    "                    rec = self.recall(label, average)\n",
    "                    prec = self.precision(label, average)\n",
    "                    \n",
    "                    if rec + prec == 0:\n",
    "                        f1_score_label = 0\n",
    "                    else:\n",
    "                        f1_score_label = 2 * float((prec * rec) / (prec + rec))\n",
    "                    if average == \"macro\":\n",
    "                        ratio = 1 / len(self.classes_)\n",
    "                    elif average == \"weighted\":\n",
    "                        ratio = Counter(self.actuals)[label] / float(n)\n",
    "                    else:\n",
    "                        raise Exception(\"unknown type of average\")\n",
    "                    \n",
    "                    f1_score += f1_score_label * ratio\n",
    "\n",
    "        \n",
    "        return f1_score\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def auc(self, target):\n",
    "        # compute AUC of ROC curve for each class\n",
    "        # return auc = {self.classes_[i]: auc_i}, dict\n",
    "        \n",
    "        if type(self.pred_proba)==type(None):\n",
    "            return None\n",
    "        else:\n",
    "            if target in self.classes_:\n",
    "                order = np.argsort(self.pred_proba[target])[::-1]\n",
    "                tp = 0\n",
    "                fp = 0\n",
    "                fn = Counter(self.actuals)[target]\n",
    "                tn = len(self.actuals) - fn\n",
    "#                 print(fn)\n",
    "#                 print(tn)\n",
    "#                 set_trace()\n",
    "                \n",
    "                # Pre-TPR calculations\n",
    "                if tp + fn == 0:\n",
    "                    tpr = 0\n",
    "                else:\n",
    "                    tpr = tp / (tp + fn)\n",
    "                \n",
    "                # pre-FPR calculations\n",
    "                if tn + fp == 0:\n",
    "                    fpr = 0\n",
    "                else:\n",
    "                    fpr = fp / (fp + tn)\n",
    "                \n",
    "                \n",
    "                auc_target = 0\n",
    "                \n",
    "                for i in order:\n",
    "                    # when order is Y\n",
    "                    if self.actuals[i] == target:\n",
    "                        tp = tp + 1\n",
    "                        fn =  Counter(self.actuals)[target]\n",
    "                        if fn == 0:\n",
    "                            tpr = 0\n",
    "                        else:\n",
    "                            tpr = tp /  fn\n",
    "                        \n",
    "                    else:\n",
    "                        # when oder is N\n",
    "                        fp = fp + 1\n",
    "                        tn = len(self.actuals) - fn\n",
    "                        pre_fpr = fpr\n",
    "                        \n",
    "                        if tn == 0:\n",
    "                            fpr = 0\n",
    "                        else:\n",
    "                            fpr = fp / (tn)\n",
    "                        \n",
    "                        if fpr != pre_fpr:\n",
    "                            auc_target += (tpr * (fpr - pre_fpr))\n",
    "            else:\n",
    "                raise Exception(\"Unknown target class.\")\n",
    "\n",
    "            return auc_target\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      "['Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-virginica'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-versicolor'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-versicolor' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-versicolor' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-versicolor' 'Iris-versicolor' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica']\n",
      "\n",
      "Confusion Matrix:\n",
      "{'Iris-setosa': {'TP': 45, 'TN': 90, 'FP': 0, 'FN': 0}, 'Iris-versicolor': {'TP': 44, 'TN': 85, 'FP': 5, 'FN': 1}, 'Iris-virginica': {'TP': 40, 'TN': 89, 'FP': 1, 'FN': 5}}\n",
      "\n",
      "\n",
      "Results:\n",
      "{'Iris-setosa': {'prec': 1.0, 'recall': 1.0, 'f1': 1.0, 'auc': 1.0}, 'Iris-versicolor': {'prec': 0.8979591836734694, 'recall': 0.9777777777777777, 'f1': 0.9361702127659575, 'auc': 0.98}, 'Iris-virginica': {'prec': 0.975609756097561, 'recall': 0.8888888888888888, 'f1': 0.9302325581395349, 'auc': 0.9587654320987653}}\n",
      "\n",
      "Average F1 scores: \n",
      "{'macro': 0.9554675903018307, 'micro': 0.9555555555555556, 'weighted': 0.9554675903018307}\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "data_train = pd.read_csv(\"../data/Iris_train.csv\")\n",
    "# Separate independent variables and dependent variables\n",
    "independent = [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]\n",
    "X = data_train[independent]\n",
    "y = data_train[\"Species\"]\n",
    "# Fit model\n",
    "clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=2)\n",
    "clf.fit(X, y)\n",
    "# Predict on training data\n",
    "predictions = clf.predict(X)\n",
    "print(\"Predictions:\")\n",
    "print(predictions)\n",
    "print()\n",
    "# Predict probabilities\n",
    "probs = clf.predict_proba(X)\n",
    "probs = pd.DataFrame({key: probs[:, i] for i, key in enumerate(clf.classes_)})\n",
    "# Evaluate results\n",
    "metrics = my_evaluation(predictions, y, probs)\n",
    "result = {}\n",
    "for target in clf.classes_:\n",
    "    result[target] = {}\n",
    "    result[target][\"prec\"] = metrics.precision(target)\n",
    "    result[target][\"recall\"] = metrics.recall(target)\n",
    "    result[target][\"f1\"] = metrics.f1(target)\n",
    "    result[target][\"auc\"] = metrics.auc(target)\n",
    "print()   \n",
    "print(\"Results:\")\n",
    "print(result)\n",
    "print()\n",
    "f1 = {average: metrics.f1(target=None, average=average) for average in [\"macro\", \"micro\", \"weighted\"]}\n",
    "print(\"Average F1 scores: \")\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
