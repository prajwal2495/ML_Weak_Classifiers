{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision\n",
    "\n",
    "\n",
    "if target in self.classes_:\n",
    "    tp = self.confusion_matrix[target][\"TP\"]\n",
    "    fp = self.confusion_matrix[target][\"FP\"]\n",
    "    if tp + fp == 0:\n",
    "        prec = 0\n",
    "    else:\n",
    "        prec = float(tp) / (tp + fp + EPS)\n",
    "else:\n",
    "    if average == \"micro\":\n",
    "        prec = self.accuracy()\n",
    "    else:\n",
    "        prec = 0\n",
    "        n = len(self.classes_)\n",
    "        for label in self.classes_:\n",
    "            tp = self.confusion_matrix[label][\"TP\"]\n",
    "            fp = self.confusion_matrix[label][\"FP\"]\n",
    "            if tp + fp == 0:\n",
    "                prec_label = 0\n",
    "            else:\n",
    "                prec_label = float(tp) / (tp + fp )\n",
    "            if average == \"macro\":\n",
    "                ratio = 1 / len(self.classes_)\n",
    "            elif average == \"weighted\":\n",
    "                ratio = Counter(self.actuals)[label] / float(n)\n",
    "            else:\n",
    "                raise Exception(\"Unknown type of average.\")\n",
    "\n",
    "            prec = prec_label  * ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re - call\n",
    "\n",
    "if target in self.classes_:\n",
    "    tp = self.confusion_matrix[target][\"TP\"]\n",
    "    fn = self.confusion_matrix[target][\"FN\"]\n",
    "    if tp + fn == 0:\n",
    "        rec = 0\n",
    "    else:\n",
    "        rec = float(tp) / (tp + fn )\n",
    "else:\n",
    "    if average == \"micro\":\n",
    "        rec = self.accuracy()\n",
    "    else:\n",
    "        rec = 0\n",
    "        n = len(self.classes_)\n",
    "        for label in self.classes_:\n",
    "            tp = self.confusion_matrix[label][\"TP\"]\n",
    "            fn = self.confusion_matrix[label][\"FN\"]\n",
    "            if tp + fn == 0:\n",
    "                rec_label = 0\n",
    "            else:\n",
    "                rec_label = float(tp) / (tp + fn )\n",
    "            if average == \"macro\":\n",
    "                ratio = 1 / len(self.classes_)\n",
    "            elif average == \"weighted\":\n",
    "                ratio = Counter(self.actuals)[label] / float(n)\n",
    "            else:\n",
    "                raise Exception(\"Unknown type of average.\")\n",
    "            rec += rec_label * ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion - matrix\n",
    "\n",
    "\n",
    "for label in self.classes_:\n",
    "    tp = (Counter(correct)[True])\n",
    "\n",
    "    fp = (Counter(correct)[False])\n",
    "\n",
    "    fn = (Counter(wrong)[False])\n",
    "    # fn = (Counter(wrong)[True] and self.actual[i] == label)\n",
    "    # fn = Counter(self.actuals)[label]\n",
    "\n",
    "    tn = (Counter(wrong)[True])\n",
    "    # tn = len(self.actuals) - fn\n",
    "\n",
    "\n",
    "    self.confusion_matrix[label] = {\"TP\":tp, \"TN\": tn, \"FP\": fp, \"FN\": fn}\n",
    "print(\"Confusion Matrix:\")\n",
    "print(self.confusion_matrix)\n",
    "return   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_target = auc_target / len(self.classes_)\n",
    "auc_target = (tpr * fpr) / 2\n",
    "auc_target = 0.5 - (((fpr / 2) + (tpr / 2)) - ((tpr * fpr) / 2))\n",
    "auc_target = tpr * (fpr - pre_fpr)\n",
    "auc_target = tpr * fpr\n",
    "auc_target = 0.5 - ((fpr / 2) + (tpr / 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(self, target=None, average = \"macro\"):\n",
    "        # compute f1\n",
    "        # target: target class (str). If not None, then return f1 of target class\n",
    "        # average: {\"macro\", \"micro\", \"weighted\"}. If target==None, return average f1\n",
    "        # output: f1 = float\n",
    "        \n",
    "        f1_target = target\n",
    "        f1_average = average    \n",
    "                        \n",
    "                \n",
    "        if self.confusion_matrix == None:\n",
    "            self.confusion()\n",
    "            \n",
    "        if target in self.classes_:\n",
    "            f1_rec = self.recall(f1_target,f1_average)\n",
    "            f1_prec = self.precision(f1_target,f1_average)\n",
    "            \n",
    "            if f1_rec + f1_prec == 0:\n",
    "                f1_score = 0\n",
    "            else:\n",
    "                f1_score = 2 * ((f1_prec * f1_rec) / (f1_prec + f1_rec)) \n",
    "        else:\n",
    "            if average == \"micro\":\n",
    "                f1_score = self.accuracy()\n",
    "            else:\n",
    "                f1_score = 0\n",
    "                n = len(self.classes_)\n",
    "                \n",
    "                for label in self.classes_:\n",
    "                    f1_rec = self.recall(f1_target,f1_average)\n",
    "                    f1_prec = self.precision(f1_target,f1_average)\n",
    "                    \n",
    "                    if f1_rec + f1_prec == 0:\n",
    "                        f1_score_label = 0\n",
    "                    else:\n",
    "                        f1_score_label = 2 * ((f1_prec * f1_rec) / (f1_prec + f1_rec))\n",
    "                    \n",
    "                    if average == \"macro\":\n",
    "                        ratio = 1 / len(self.classes_)\n",
    "                    \n",
    "                    elif average = \"weighted\":\n",
    "                        ratio = Counter(self.actuals)[label] / float(n)\n",
    "                    else:\n",
    "                        raise Exception(\"unknown type of average\")\n",
    "                    \n",
    "                    f1_score = f1_score_label * ratio\n",
    "        \n",
    "        return f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_target = target\n",
    "f1_average = average\n",
    "\n",
    "if self.confusion_matrix == None:\n",
    "    self.confusion()\n",
    "\n",
    "if f1_target in self.classes_:\n",
    "    f1_recall = self.recall(f1_target, f1_average)\n",
    "    f1_precision = self.precision(f1_target, f1_average)\n",
    "    if f1_recall + f1_precision == 0:\n",
    "        f1_score = 0\n",
    "    else:\n",
    "        f1_score = 2 * ((f1_precision * f1_recall) / (f1_precision + f1_recall))\n",
    "\n",
    "else:\n",
    "    if f1_average == \"micro\":\n",
    "         f1_score = self.accuracy()\n",
    "    else:\n",
    "        f1_score = 0\n",
    "        n = len(self.classes_)\n",
    "\n",
    "        for label in self.classes_:\n",
    "            f1_recall = self.recall(f1_target, f1_average)\n",
    "            f1_precision = self.precision(f1_target, f1_average)\n",
    "\n",
    "            if f1_recall + f1_precision == 0:\n",
    "                f1_score_label = 0\n",
    "            else:\n",
    "                f1_score_label = 2 * ((f1_precision * f1_recall) / (f1_precision + f1_recall))\n",
    "            if average == \"macro\":\n",
    "                ratio = 1 / len(self.classes_)\n",
    "                f1_score_label = 2 *((self.precision(f1_target,average) * self.recall(f1_target,average)) / (self.precision(f1_target,average) + self.recall(f1_target, average)))\n",
    "            elif average == \"weighted\":\n",
    "                ratio = Counter(self.actuals)[label] / float(n)\n",
    "                f1_score_label = 2 *((self.precision(f1_target,average) * self.recall(f1_target,average)) / (self.precision(f1_target,average) + self.recall(f1_target, average)))\n",
    "            else:\n",
    "                raise Exception(\"Unknown type of average\")\n",
    "\n",
    "            f1_score = (f1_score_label * ratio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
