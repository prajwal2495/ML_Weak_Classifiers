{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from pdb import set_trace\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_AdaBoost:\n",
    "\n",
    "    def __init__(self, base_estimator = None, n_estimators = 50):\n",
    "        # base_estimator: the base classifier class, e.g. my_DT\n",
    "        # n_estimators: # of base_estimator rounds\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_estimators = int(n_estimators)\n",
    "        self.estimators = [deepcopy(self.base_estimator) for i in range(self.n_estimators)]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # X: pd.DataFrame, independent variables, float\n",
    "        # y: list, np.array or pd.Series, dependent variables, int or str\n",
    "\n",
    "        self.classes_ = list(set(list(y)))\n",
    "        k = len(self.classes_)\n",
    "        n = len(y)\n",
    "        w = np.array([1.0 / n] * n)\n",
    "        labels = np.array(y)\n",
    "        self.alpha = []\n",
    "        for i in range(self.n_estimators):\n",
    "            # Sample with replacement from X, with probability w\n",
    "            sample = np.random.choice(n, n, p=w)\n",
    "            # Train base classifier with sampled training data\n",
    "            sampled = X.iloc[sample]\n",
    "            sampled.index = range(len(sample))\n",
    "            self.estimators[i].fit(sampled, labels[sample])\n",
    "            predictions = self.estimators[i].predict(X)\n",
    "            diffs = np.array(predictions) != y\n",
    "            # Compute error rate and alpha for estimator i\n",
    "            error = np.sum(diffs * w)\n",
    "            while error >= (1 - 1.0 / k):\n",
    "                w = np.array([1.0 / n] * n)\n",
    "                sample = np.random.choice(n, n, p=w)\n",
    "                # Train base classifier with sampled training data\n",
    "                sampled = X.iloc[sample]\n",
    "                sampled.index = range(len(sample))\n",
    "                self.estimators[i].fit(sampled, labels[sample])\n",
    "                predictions = self.estimators[i].predict(X)\n",
    "                diffs = np.array(predictions) != y\n",
    "                # Compute error rate and alpha for estimator i\n",
    "                error = np.sum(diffs * w)\n",
    "            # Compute alpha for estimator i\n",
    "            EPS = 1e-10\n",
    "            log_k = np.log(1 - 1.0 / k)\n",
    "            log_error = np.log((1 - error) / (error + EPS))\n",
    "            self.alpha.append(log_k + log_error)\n",
    "            #self.alpha.append((np.log(k - 1)) + (np.log(1 - error) / (error + EPS)))\n",
    "            #print(range(len(diffs)))\n",
    "\n",
    "            # Update wi\n",
    "            for i in range(len(diffs)):\n",
    "                if(diffs[i]):\n",
    "                    w[i] *= np.exp(self.alpha[-1])\n",
    "                    #w[i] = w[i]\n",
    "#                 else:\n",
    "#                     #w[i] *= np.exp(self.alpha[-1] * diffs[i])\n",
    "#                     w[i] = w[i]\n",
    "            \n",
    "            \n",
    "            #normalisation of w\n",
    "            w = w / np.sum(w)    \n",
    "            #print(np.sum(w))\n",
    "            #w = \"write your own code\"\n",
    "\n",
    "        # Normalize alpha\n",
    "        self.alpha = self.alpha / np.sum(self.alpha)\n",
    "        return\n",
    "\n",
    "    def predict(self, X):\n",
    "        # X: pd.DataFrame, independent variables, float\n",
    "        # return predictions: list\n",
    "        probs = self.predict_proba(X)\n",
    "        predictions = [self.classes_[np.argmax(prob)] for prob in probs.to_numpy()]\n",
    "        return predictions\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        # X: pd.DataFrame, independent variables, float\n",
    "        # prob: what percentage of the base estimators predict input as class C\n",
    "        # prob(x)[C] = sum(alpha[j] * (base_model[j].predict(x) == C))\n",
    "        # return probs = pd.DataFrame(list of prob, columns = self.classes_)\n",
    "        # write your code below\n",
    "        probs = {}\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        for label in self.classes_:\n",
    "            runsum = 0\n",
    "            for i in range(self.n_estimators):\n",
    "                runsum += ((self.alpha[i]) * (self.estimators[i].predict(X) == label))\n",
    "            probs[label] = (runsum)\n",
    "                  \n",
    "                \n",
    "#         for labels_i,values_i in probs.items():\n",
    "#             probs[labels_i] = (np.sum(probs[labels_i][values_i]))\n",
    "                \n",
    "                \n",
    "        probs = pd.DataFrame(probs, columns=self.classes_)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-fce0aca5e786>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindependent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;31m# Predict probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-79fb097f7c12>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m# X: pd.DataFrame, independent variables, float\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# return predictions: list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-79fb097f7c12>\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0madd_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0madd_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'float'"
     ]
    }
   ],
   "source": [
    "#  Load training data\n",
    "#  Load training data\n",
    "data_train = pd.read_csv(\"../data/Iris_train.csv\")\n",
    "# Separate independent variables and dependent variables\n",
    "independent = [\"SepalLengthCm\",\t\"SepalWidthCm\",\t\"PetalLengthCm\",\t\"PetalWidthCm\"]\n",
    "X = data_train[independent]\n",
    "y = data_train[\"Species\"]\n",
    "# Train model\n",
    "base_estimator = DecisionTreeClassifier(criterion = \"entropy\", max_depth = 1)\n",
    "clf = my_AdaBoost(base_estimator=base_estimator, n_estimators = 10)\n",
    "clf.fit(X, y)\n",
    "# Load testing data\n",
    "data_test = pd.read_csv(\"../data/Iris_test.csv\")\n",
    "X_test = data_test[independent]\n",
    "# Predict\n",
    "predictions = clf.predict(X_test)\n",
    "# Predict probabilities\n",
    "probs = clf.predict_proba(X_test)\n",
    "# Print results\n",
    "for i, pred in enumerate(predictions):\n",
    "    print(\"%s\\t%f\" % (pred, probs[pred][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
