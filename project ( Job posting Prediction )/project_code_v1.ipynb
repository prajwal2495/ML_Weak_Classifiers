{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier,AdaBoostRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.utils import resample \n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "from assignment8.my_evaluation import my_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_model():\n",
    "    def fit(self, X, y):\n",
    "        # do not exceed 29 mins\n",
    "        self.y_data = y\n",
    "        \n",
    "        #count vectorizing description\n",
    "        description_X = X.description\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(description_X, y, test_size = 0.33, shuffle = True)\n",
    "        \n",
    "        count_vector = CountVectorizer(stop_words='english').fit(X_train)\n",
    "        \n",
    "        data_frame_desc_train = pd.DataFrame(count_vector.transform(X_train).todense(), \n",
    "                                       columns = count_vector.get_feature_names())\n",
    "        \n",
    "        data_frame_desc_test = pd.DataFrame(count_vector.transform(X_test).todense(),\n",
    "                                      columns=count_vector.get_feature_names())\n",
    "        \n",
    "        ######################Count vectorizing description end ########################\n",
    "        \n",
    "        #count vectorizing requirements\n",
    "        req_x = X.requirements\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(req_x, y, test_size = 0.33, shuffle = True)\n",
    "        \n",
    "        data_frame_req_train = pd.DataFrame(count_vector.transform(X_train).todense(),\n",
    "                                             columns=count_vector.get_feature_names())\n",
    "        \n",
    "        data_frame_req_test = pd.DataFrame(count_vector.transform(X_test).todense(),\n",
    "                                            columns=count_vector.get_feature_names())\n",
    "        \n",
    "                ######################Count vectorizing requirements end ########################\n",
    "        \n",
    "        #count vectorizing requirements\n",
    "#         title_x = X.title\n",
    "#         X_train, X_test, y_train, y_test = train_test_split(title_x, y, test_size = 0.33, shuffle = True)\n",
    "        \n",
    "#         data_frame_title_train = pd.DataFrame(count_vector.transform(X_train).todense(),\n",
    "#                                              columns=count_vector.get_feature_names())\n",
    "        \n",
    "#         data_frame_title_test = pd.DataFrame(count_vector.transform(X_test).todense(),\n",
    "#                                             columns=count_vector.get_feature_names())\n",
    "         \n",
    "            ######################Count vectorizing requirements end ########################\n",
    "    \n",
    "        \n",
    "        #concatenate all the vectorized data frames \n",
    "        training = pd.concat([data_frame_desc_train, data_frame_req_train], axis=1)\n",
    "        testing = pd.concat([data_frame_desc_test,data_frame_req_test], axis=1)\n",
    "        \n",
    "        print(\"after balancing the data:\")\n",
    "        print(training.shape)\n",
    "        print(y_train.shape)\n",
    "        print()\n",
    "        print(testing.shape)\n",
    "        print(y_test.shape)\n",
    "        \n",
    "        #set_trace()\n",
    "        \n",
    "        parameter_grid = {\"base_estimator__criterion\" : [\"gini\",\"entropy\"],\n",
    "                         \"base_estimator__splitter\" : [\"best\", \"random\"],\n",
    "                         \"n_estimators\" : [1,2]}\n",
    "\n",
    "\n",
    "        param_dist = {\n",
    "        'n_estimators': [50, 100],\n",
    "        'learning_rate' : [0.01,0.05,0.1,0.3,1],\n",
    "        'loss' : ['linear', 'square', 'exponential']\n",
    "        }\n",
    "\n",
    "        pre_gs_inst = RandomizedSearchCV(AdaBoostRegressor(),\n",
    "        param_distributions = param_dist,\n",
    "        cv=3,\n",
    "        n_iter = 10,\n",
    "        n_jobs=-1)\n",
    "\n",
    "        pre_gs_inst.fit(training, y_train)\n",
    "        predictions = pre_gs_inst.predict(testing)\n",
    "\n",
    "#         tree = DecisionTreeClassifier(max_depth=15)\n",
    "#         tree.fit(training, y_train)\n",
    "#         tree.score(testing, y_test)\n",
    "#         predictions = tree.predict(testing)\n",
    "        \n",
    "    \n",
    "#         GNB = GaussianNB()\n",
    "#         GNB.fit(training,y_train)\n",
    "#         GNB.score(testing,y_test)\n",
    "#         predictions = GNB.predict(testing)\n",
    "\n",
    "        \n",
    "\n",
    "#         boost = AdaBoostClassifier(n_estimators=100,random_state=0)\n",
    "#         boost.fit(training, y_train)\n",
    "#         boost.score(testing, y_test)\n",
    "#         predictions = boost.predict(testing)\n",
    "        \n",
    "#         evaluate = my_evaluation(predictions, y_test)\n",
    "#         f1_score = evaluate.f1(target = 1)\n",
    "        \n",
    "#         print()\n",
    "#         print(\"this is eval f1 in fit\", f1_score)\n",
    "#         print()\n",
    "        \n",
    "        print(\"fit method Classification report:\")\n",
    "        print(metrics.classification_report(y_test,predictions))\n",
    "        \n",
    "        return\n",
    "\n",
    "    def predict(self, X):\n",
    "        #count vectorizing description\n",
    "        description_X = X.description\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(description_X, self.y_data, test_size = 0.33, shuffle = True)\n",
    "        \n",
    "        count_vector = CountVectorizer(stop_words='english').fit(X_train)\n",
    "        \n",
    "        data_frame_desc_train = pd.DataFrame(count_vector.transform(X_train).todense(), \n",
    "                                       columns = count_vector.get_feature_names())\n",
    "        \n",
    "        data_frame_desc_test = pd.DataFrame(count_vector.transform(X_test).todense(),\n",
    "                                      columns=count_vector.get_feature_names())\n",
    "        \n",
    "        ######################Count vectorizing description end ########################\n",
    "        \n",
    "        #count vectorizing requirements\n",
    "        req_x = X.requirements\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(req_x, self.y_data, test_size = 0.33, shuffle = True)\n",
    "        \n",
    "        data_frame_req_train = pd.DataFrame(count_vector.transform(X_train).todense(),\n",
    "                                             columns=count_vector.get_feature_names())\n",
    "        \n",
    "        data_frame_req_test = pd.DataFrame(count_vector.transform(X_test).todense(),\n",
    "                                            columns=count_vector.get_feature_names())\n",
    "        \n",
    "                ######################Count vectorizing requirements end ########################\n",
    "        \n",
    "        #count vectorizing requirements\n",
    "#         title_x = X.title\n",
    "#         X_train, X_test, y_train, y_test = train_test_split(title_x, y, test_size = 0.33, shuffle = True)\n",
    "        \n",
    "#         data_frame_title_train = pd.DataFrame(count_vector.transform(X_train).todense(),\n",
    "#                                              columns=count_vector.get_feature_names())\n",
    "        \n",
    "#         data_frame_title_test = pd.DataFrame(count_vector.transform(X_test).todense(),\n",
    "#                                             columns=count_vector.get_feature_names())\n",
    "         \n",
    "            ######################Count vectorizing requirements end ########################\n",
    "    \n",
    "        \n",
    "        #concatenate all the vectorized data frames \n",
    "        training = pd.concat([data_frame_desc_train, data_frame_req_train], axis=1)\n",
    "        testing = pd.concat([data_frame_desc_test,data_frame_req_test], axis=1)\n",
    "\n",
    "#         tree = DecisionTreeClassifier(max_depth=15)\n",
    "#         tree.fit(training, y_train)\n",
    "#         tree.score(testing, y_test)\n",
    "#         predictions = tree.predict(testing)\n",
    "        \n",
    "        boost = AdaBoostClassifier(n_estimators=50,random_state=0)\n",
    "        boost.fit(training, y_train)\n",
    "        boost.score(testing, y_test)\n",
    "        predictions = boost.predict(testing)\n",
    "        \n",
    "        evaluate = my_evaluation(predictions, y_test)\n",
    "        f1_score = evaluate.f1(target = 1)\n",
    "        \n",
    "        print(\"this is eval f1 in test\", f1_score)\n",
    "        print()\n",
    "        \n",
    "        print(\"test method Classification report:\")\n",
    "        print(metrics.classification_report(y_test,predictions))\n",
    "        print()\n",
    "        \n",
    "        \n",
    "#         GNB = GaussianNB()\n",
    "#         GNB.fit(training,y_train)\n",
    "#         GNB.score(testing,y_test)\n",
    "#         predictions = GNB.predict(testing)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def clean_data_frame(self,data_frame):\n",
    "                \n",
    "        #fillna to location column\n",
    "        data_frame['location'] = data_frame.location.fillna('none')\n",
    "\n",
    "        #fillna to description column\n",
    "        data_frame['description'] = data_frame.description.fillna('not specified')\n",
    "\n",
    "        #fillna to requirements column\n",
    "        data_frame['requirements'] = data_frame.description.fillna('not specified')\n",
    "        \n",
    "        #drop unnecassary columns\n",
    "        data_frame.drop(['telecommuting','has_questions'],axis = 1, inplace = True)  \n",
    "        \n",
    "        #mapping fraudulent to T and F, where there is  0 and 1 respectively\n",
    "        data_frame['has_company_logo'] = data_frame.has_company_logo.map({1 : 't', 0 : 'f'})\n",
    "        data_frame['fraudulent'] = data_frame.fraudulent.map({1 : 't', 0 : 'f'})\n",
    "        \n",
    "        #remove any unnecassary web tags in the data set\n",
    "        data_frame['title'] = data_frame.title.str.replace(r'<[^>]*>', '')\n",
    "        data_frame['description'] = data_frame.description.str.replace(r'<[^>]*>', '')\n",
    "        data_frame['requirements'] = data_frame.requirements.str.replace(r'<[^>]*>', '')\n",
    "        \n",
    "        # removing the characters in data set that are not words and has white spaces \n",
    "        for column in data_frame.columns:\n",
    "            data_frame[column] = data_frame[column].str.replace(r'\\W', ' ').str.replace(r'\\s$','')\n",
    "            \n",
    "        # mapping back the columns to original binary values\n",
    "        #data_frame['has_company_logo'] = data_frame.has_company_logo.map({'t': 1, 'f':0})\n",
    "        data_frame['fraudulent'] = data_frame.fraudulent.map({'t': 1, 'f':0})\n",
    "        \n",
    "        #add all STOPWORDS from genism to a list\n",
    "        self.all_gensim_stop_words = STOPWORDS\n",
    "\n",
    "        #adding all the columns in the data_frame to a list\n",
    "        text_columns = list(data_frame.columns.values)\n",
    "        text_columns.remove('fraudulent')\n",
    "\n",
    "        #cleaning all the columns by removing the stopwords in each of them\n",
    "        for columns in text_columns:\n",
    "            self.clean_all_columns(data_frame,columns)\n",
    "        \n",
    "        # as 1 and 0 values in the fraudulent class is highly unbalanced\n",
    "        # true = 0 and fake = 1\n",
    "        # 0 : 1 == 8484 : 456\n",
    "        Class_1 = data_frame[data_frame.fraudulent == 1]\n",
    "        Class_0 = data_frame[data_frame.fraudulent == 0]\n",
    "\n",
    "        Class_0_count, Class_1_count = data_frame.fraudulent.value_counts()\n",
    "\n",
    "        Class_0_undersampling = Class_0.sample(Class_1_count - 150)\n",
    "        #Class_0_undersampling = Class_0.sample(400)\n",
    "\n",
    "\n",
    "        data_frame_undersample = pd.concat([Class_0_undersampling, Class_1], axis=0)\n",
    "        \n",
    "        \n",
    "        return data_frame_undersample\n",
    "    \n",
    "    def clean_all_columns(self,data_frame,column_name):\n",
    "        data_frame[column_name] = data_frame[column_name].apply(lambda x: \" \".join([i for i in x.lower().split() if i not in self.all_gensim_stop_words]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after balancing the data:\n",
      "(1059, 23068)\n",
      "(1059,)\n",
      "\n",
      "(523, 23068)\n",
      "(523,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter loss for estimator AdaBoostClassifier(n_estimators=100). Check the list of available parameters with `estimator.get_params().keys()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/prajwal/env/base/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py\", line 431, in _process_worker\n    r = call_item()\n  File \"/home/prajwal/env/base/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py\", line 285, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/home/prajwal/env/base/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 595, in __call__\n    return self.func(*args, **kwargs)\n  File \"/home/prajwal/env/base/lib/python3.8/site-packages/joblib/parallel.py\", line 252, in __call__\n    return [func(*args, **kwargs)\n  File \"/home/prajwal/env/base/lib/python3.8/site-packages/joblib/parallel.py\", line 252, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"/home/prajwal/env/base/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 520, in _fit_and_score\n    estimator = estimator.set_params(**cloned_parameters)\n  File \"/home/prajwal/env/base/lib/python3.8/site-packages/sklearn/base.py\", line 249, in set_params\n    raise ValueError('Invalid parameter %s for estimator %s. '\nValueError: Invalid parameter loss for estimator AdaBoostClassifier(n_estimators=100). Check the list of available parameters with `estimator.get_params().keys()`.\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-20988436f657>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mruntime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m60.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-39890f393a6f>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     75\u001b[0m         n_jobs=-1)\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mpre_gs_inst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_gs_inst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/base/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/base/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/base/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1527\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1528\u001b[0m         \u001b[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1529\u001b[0;31m         evaluate_candidates(ParameterSampler(\n\u001b[0m\u001b[1;32m   1530\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m             random_state=self.random_state))\n",
      "\u001b[0;32m~/env/base/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    706\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[1;32m    709\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/base/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/base/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/base/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid parameter loss for estimator AdaBoostClassifier(n_estimators=100). Check the list of available parameters with `estimator.get_params().keys()`."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    start = time.time()\n",
    "\n",
    "    # Load data\n",
    "    data = pd.read_csv(\"../data/fake_job_postings.csv\")\n",
    "    clf = my_model()\n",
    "\n",
    "    # Replace missing values with empty strings\n",
    "    data = data.fillna(\"\")\n",
    "\n",
    "    data = clf.clean_data_frame(data)\n",
    "#     f1 = evaluate_data(data)\n",
    "#     print(\"F1:\",f1)\n",
    "\n",
    "    y = data[\"fraudulent\"]\n",
    "    X = data.drop(['fraudulent'], axis=1)\n",
    "\n",
    "    # Train model\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    runtime = (time.time() - start) / 60.0\n",
    "    print(\"Total Runtime:\",runtime)\n",
    "    print()\n",
    "\n",
    "    predictions = clf.predict(X)\n",
    "    print(\"Predictions:\")\n",
    "    print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
